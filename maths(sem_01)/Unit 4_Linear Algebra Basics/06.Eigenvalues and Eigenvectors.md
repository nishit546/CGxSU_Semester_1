# ðŸ“˜ Eigenvalues and Eigenvectors â€“ Definition, Computation & Applications

---

## ðŸ”· 1. Introduction to Eigenvalues and Eigenvectors ðŸ§­

### âœ… **Definition**

- An **eigenvector** of a square matrix \(A\) is a **non-zero vector** that, when multiplied by \(A\), **changes only in magnitude but not in direction**.  
- The scalar by which it is stretched or compressed is called the **eigenvalue**.  

Mathematically:

\[
A\vec{v} = \lambda \vec{v}
\]

Where:

- \(A\) = square matrix  
- \(\vec{v}\) = eigenvector (\(\vec{v} \neq 0\))  
- \(\lambda\) = eigenvalue (scalar)  

ðŸ“Œ **Key Concept:**  
- If \(\lambda > 0\): direction unchanged, magnitude scaled  
- If \(\lambda < 0\): direction reversed, magnitude scaled  
- Eigenvectors are **never zero vectors**  

---

### âœ… **Geometric Intuition**

Original vector v â”€â”€â”€â”€â”€â–¶
After transformation A:
Î»v â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ (same direction, different length)


- **Direction unchanged** â†’ eigenvector  
- **Scaling factor** â†’ eigenvalue  

---

## ðŸ”· 2. Characteristic Equation

- To find eigenvalues \(\lambda\):

\[
\det(A - \lambda I) = 0
\]

Where:

- \(I\) = identity matrix (same order as \(A\))  
- \(\lambda\) = eigenvalue  

- Solving this gives a **polynomial equation** (quadratic for 2Ã—2, cubic for 3Ã—3, etc.)  
- **Eigenvectors** are found by solving:

\[
(A - \lambda I)\vec{v} = 0
\]

---

## ðŸ”· 3. Step-by-Step Procedure to Find Eigenvalues and Eigenvectors

1. Compute eigenvalues \(\lambda\) using \(\det(A - \lambda I) = 0\)  
2. Substitute each \(\lambda\) into \((A - \lambda I)\vec{v} = 0\)  
3. Solve the linear system for \(\vec{v}\)  
4. Multiple solutions exist â€” any scalar multiple of an eigenvector is also an eigenvector  

---

## ðŸ”· 4. Examples

### âœ… **Example 1: 2Ã—2 Matrix**

\[
A = \begin{bmatrix}2 & 1 \\ 1 & 2\end{bmatrix}
\]

**Step 1: Characteristic Equation**

\[
\det(A - \lambda I) = 
\begin{vmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{vmatrix} = 0
\]

\[
(2-\lambda)^2 - 1 = 0 \implies \lambda^2 - 4\lambda + 3 = 0
\]

\[
(\lambda-3)(\lambda-1) = 0
\]

âœ… **Eigenvalues:** \(\lambda = 3, 1\)

**Step 2: Eigenvectors**

- For \(\lambda = 3\):

\[
(A-3I)\vec{v} = 0 \implies 
\begin{bmatrix}-1 & 1 \\ 1 & -1\end{bmatrix} 
\begin{bmatrix}x \\ y\end{bmatrix} = 0
\]

\[
x = y \implies \vec{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}
\]

- For \(\lambda = 1\):

\[
(A-I)\vec{v} = 0 \implies 
\begin{bmatrix}1 & 1 \\ 1 & 1\end{bmatrix} 
\begin{bmatrix}x \\ y\end{bmatrix} = 0
\]

\[
x = -y \implies \vec{v} = \begin{bmatrix}1 \\ -1\end{bmatrix}
\]

âœ… **Answer:**  
- Eigenvalues: 3, 1  
- Eigenvectors: \([1,1], [1,-1]\)

---

### âœ… **Example 2: 3Ã—3 Matrix**

\[
A = \begin{bmatrix}2 & 2 & 2 \\ 2 & 2 & 2 \\ 2 & 2 & 2\end{bmatrix}
\]

**Step 1: Characteristic Equation**

\[
\det(A - \lambda I) = 0 \implies 
\lambda^2(6-\lambda) = 0
\]

âœ… **Eigenvalues:** \(\lambda = 6, 0, 0\)

**Step 2: Eigenvectors**

- For \(\lambda = 6\):

\[
(A-6I)\vec{v} = 0 \implies \vec{v} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}
\]

- For \(\lambda = 0\):

\[
(A-0I)\vec{v} = 0 \implies a + b + c = 0
\]

- Choose free variables:

\[
\vec{v}_1 = \begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix}, \quad
\vec{v}_2 = \begin{bmatrix}-1 \\ 0 \\ 1\end{bmatrix}
\]

âœ… **Answer:**  
- Eigenvalues: 6, 0, 0  
- Eigenvectors: \([1,1,1], [-1,1,0], [-1,0,1]\)

---

## ðŸ”· 5. Applications of Eigenvalues and Eigenvectors

- **Google PageRank** â€“ importance of web pages  
- **Markov Processes** â€“ steady-state probabilities  
- **Principal Component Analysis (PCA)** â€“ dimensionality reduction, feature extraction  
- **Latent Semantic Analysis (LSA)** â€“ NLP, document similarity  
- **Spectral Graph Theory** â€“ community detection, connectivity analysis  
- **Computer Vision** â€“ Eigenfaces for face recognition  
- **Control Systems & Robotics** â€“ stability analysis  
- **Signal Processing** â€“ filtering, noise reduction, feature extraction  

---

## ðŸ”· 6. Key Points to Remember

- Number of eigenvalues = order of matrix  
- Eigenvectors are never zero vectors  
- Diagonal matrices â†’ eigenvalues are the diagonal entries  
- Eigenvectors can have multiple scalar multiples  
- Negative eigenvalues reverse direction of eigenvectors  

---

