
# ğŸ“˜ **Eigenvalues and Eigenvectors â€“ Definition, Computation & Applications**

---

## ğŸ”· **1. Eigenvalues and Eigenvectors â€“ Detailed Definition** ğŸ§­

### âœ… **Detailed Definition (**

An **eigenvector** of a square matrix (A) is a **non-zero vector** that, when multiplied by (A), **changes only in magnitude but not in direction**. The scalar by which it is stretched or compressed is called the **eigenvalue**. Mathematically, if $(\vec{v})$ is an eigenvector and $(\lambda)$ is its eigenvalue, then
$$[
A\vec{v} = \lambda \vec{v}
]$$
This means that the transformation represented by (A) acts on $(\vec{v})$ by simply scaling it. Eigenvalues and eigenvectors reveal the **hidden structure of linear transformations**. They play a major role in **engineering, physics, computer science, machine learning, vibration analysis, stability of systems, Google PageRank, and Principal Component Analysis (PCA)**.

---

### âœ… **Geometric Visual Representation (Text-Based)**

```
Original vector v  â”€â”€â”€â”€â”€â–¶
After transformation A:
Î»v  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   (same direction, different length)
```

ğŸ“Œ **Direction unchanged â†’ Eigenvector**
ğŸ“Œ **Scaling factor â†’ Eigenvalue**

---

## ğŸŸ¢ **Basic Examples (Conceptual Understanding)**

---

### âœ… **Example 1 (Identity Matrix)**

[
A=\begin{bmatrix}1&0\0&1\end{bmatrix}
]

* For any vector (\vec{v}),
  (A\vec{v}=\vec{v})
* So every vector is an eigenvector
* Eigenvalue = **1**

âœ… **Conclusion:** All vectors are eigenvectors with (\lambda=1)

---

### âœ… **Example 2 (Scaling Matrix)**

[
A=\begin{bmatrix}2&0\0&3\end{bmatrix}
]

* x-axis vector â†’ scaled by 2
* y-axis vector â†’ scaled by 3

âœ… **Eigenvectors:** ((1,0)), ((0,1))
âœ… **Eigenvalues:** (2, 3)

---

### âœ… **Example 3 (Physical Meaning)**

* In vibration systems:

  * Eigenvectors â†’ **mode shapes**
  * Eigenvalues â†’ **natural frequencies**

âœ… Used in structural engineering and mechanics âš™ï¸

---

---

## ğŸ”· **2. Computation of Eigenvalues and Eigenvectors for a 2Ã—2 Matrix** ğŸ§®

---

### âœ… **Detailed Explanation (80+ Words)**

To compute the eigenvalues of a **2Ã—2 matrix**, we solve the **characteristic equation**
[
\det(A - \lambda I)=0
]
where (I) is the identity matrix and (\lambda) is the eigenvalue. This gives a **quadratic equation** whose solutions are the eigenvalues. Once the eigenvalues are found, we substitute each (\lambda) into
[
(A - \lambda I)\vec{v}=0
]
and solve the resulting linear equations to obtain the corresponding **eigenvectors**. This process helps determine the **special directions** that remain unchanged under the transformation. Eigen computation is fundamental in **diagonalization, PCA, stability analysis, and Markov processes**.

---

### âœ… **General Form**

[
A=\begin{bmatrix}a&b\c&d\end{bmatrix}
]
[
\det(A-\lambda I)=
\begin{vmatrix}
a-\lambda & b\
c & d-\lambda
\end{vmatrix}
=0
]

---

### âœ… **Step-by-Step Visual Flow (Text)**

```
Matrix A
   â”‚
   â–¼
A âˆ’ Î»I
   â”‚
   â–¼
det(A âˆ’ Î»I) = 0
   â”‚
   â–¼
Eigenvalues Î»
   â”‚
   â–¼
(A âˆ’ Î»I)v = 0
   â”‚
   â–¼
Eigenvectors v
```

---

## ğŸŸ¢ **Solved Numerical Examples (2Ã—2 Matrices)** âœ…

---

### âœ… **Example 1**

[
A=\begin{bmatrix}2&1\1&2\end{bmatrix}
]

**Step 1: Characteristic Equation**
[
\begin{vmatrix}2-\lambda & 1\1 & 2-\lambda\end{vmatrix}=0
]
[
(2-\lambda)^2-1=0
]
[
\lambda^2-4\lambda+3=0
]
[
(\lambda-3)(\lambda-1)=0
]

âœ… **Eigenvalues:** (\lambda=3, 1)

**Step 2: Eigenvectors**

For (\lambda=3):
[
(A-3I)\vec{v}=0 \Rightarrow
\vec{v}=\begin{bmatrix}1\1\end{bmatrix}
]

For (\lambda=1):
[
(A-I)\vec{v}=0 \Rightarrow
\vec{v}=\begin{bmatrix}1\-1\end{bmatrix}
]

âœ… **Final Answer:**
Eigenvalues = **3, 1**
Eigenvectors = ((1,1),\ (1,-1))

---

### âœ… **Example 2**

[
A=\begin{bmatrix}4&0\0&2\end{bmatrix}
]

* Eigenvalues = **4, 2**
* Eigenvectors = ((1,0),\ (0,1))

âœ… Diagonal matrix â†’ Eigenvalues directly from diagonal

---

### âœ… **Example 3**

[
A=\begin{bmatrix}1&2\2&1\end{bmatrix}
]

[
\det(A-\lambda I)=
\lambda^2-2\lambda-3=0
]
[
(\lambda-3)(\lambda+1)=0
]

âœ… **Eigenvalues:** (3, -1)

âœ… Directions stretch and flip due to negative eigenvalue ğŸ”„

---

### ğŸ“ **Key Points**

âœ… Number of eigenvalues = matrix order
âœ… Eigenvectors are **never zero vectors**
âœ… Diagonal matrices â†’ direct eigenvalues
âœ… Used in diagonalization & transformations

---

---

## ğŸ”· **3. Applications of Eigenvalues & Eigenvectors in PageRank and PCA** ğŸš€

---

### âœ… **Detailed Explanation (80+ Words)**

Eigenvalues and eigenvectors are the **mathematical backbone of modern data science and machine learning**. In **Googleâ€™s PageRank algorithm**, the importance of a webpage is computed using the **dominant eigenvector** of a huge matrix representing web links. This eigenvector gives the **steady-state ranking** of pages. In **Principal Component Analysis (PCA)**, eigenvectors of the **covariance matrix** determine the **principal directions** of data, while eigenvalues measure the **variance captured** in each direction. Thus, eigenvalues help decide which features are most important, and eigenvectors determine the **new coordinate system for data reduction**. These ideas are heavily used in **AI, image compression, recommendation systems, and signal processing**.

---

### âœ… **PageRank â€“ Visual Meaning (Text-Based)**

```
Web Pages â†’ Transition Matrix â†’ Eigenvector
                         â”‚
                         â–¼
                 Page Importance Ranking
```

ğŸ“Œ Largest eigenvalue = **1**
ğŸ“Œ Corresponding eigenvector = **Google PageRank scores**

---

### âœ… **PCA â€“ Visual Meaning (Text-Based)**

```
Original Data
     â”‚
     â–¼
Covariance Matrix
     â”‚
     â–¼
Eigenvectors â†’ New Axes (Principal Components)
     â”‚
     â–¼
Dimension Reduction
```

ğŸ“Œ Larger eigenvalue â†’ more variance
ğŸ“Œ Eigenvector â†’ direction of maximum data spread

---

## ğŸŸ¢ **Simple Application Examples** âœ…

---

### âœ… **Example 1 (PageRank Concept)**

* Web links are stored in a matrix (M)
* PageRank vector (\vec{p}) satisfies:
  [
  M\vec{p} = \vec{p}
  ]
* This is an **eigenvector equation with (\lambda=1)**

âœ… The steady-state ranking = dominant eigenvector

---

### âœ… **Example 2 (PCA Insight)**

* Data matrix â†’ covariance matrix (C)
* Largest eigenvalue of (C) â†’ most important feature direction
* Corresponding eigenvector â†’ first **principal component**

âœ… Used in face recognition & image compression ğŸ“·

---

### âœ… **Example 3 (Engineering Stability)**

* If all eigenvalues of system matrix are **positive** â†’ system is stable âœ…
* If any eigenvalue is **negative** â†’ system becomes unstable âš ï¸

âœ… Used in control systems & robotics ğŸ¤–

---


