# Streaming in LangGraph

Streaming is the ability to observe an agentâ€™s output **as it is being generated**, instead of waiting for the entire workflow to finish before seeing any results. This capability is especially critical for **long-running, multi-step, or interactive agentic workflows**, where waiting for a final result can feel slow, opaque, or even risky.

In traditional LLM applications, execution is usually synchronous and blocking:
1. A prompt is sent
2. The system waits
3. A full response appears at the end

During this time, users and developers have **no visibility** into what the model is doing. If something goes wrong, you only discover it after the entire process has completedâ€”or failed.

LangGraph changes this model fundamentally.

---

## What Streaming Means in LangGraph

In LangGraph, streaming is not limited to text output. Because LangGraph is built around **explicit nodes, edges, and state**, streaming can occur at multiple levels of the system.

Streaming in LangGraph can include:

- **Token-by-token LLM output**  
  See text generated by the model as it is produced.

- **Node-by-node execution updates**  
  Observe which node is currently running and when execution moves to the next step.

- **Intermediate state updates**  
  Inspect how state evolves over time as nodes write new information.

This makes LangGraph workflows **observable while they are running**, not just after they finish.

---

## Why Streaming Matters for Agentic Systems

Agentic AI systems are inherently **multi-step and dynamic**. They plan, act, evaluate, loop, and sometimes wait for human input. Streaming is essential in these systems for several reasons.

### 1. Improved User Experience

For users, streaming provides:
- Immediate feedback
- Visible progress
- Reduced perceived latency
- Confidence that the system is working

Instead of staring at a loading screen, users can watch the agent think, act, and progress toward the goal.

---

### 2. Real-Time Developer Visibility

For developers and operators, streaming enables:
- Live monitoring of agent behavior
- Insight into reasoning paths
- Detection of unexpected loops or failures
- Faster debugging and iteration

Because LangGraph workflows are explicit, streamed updates map directly to:
- Specific nodes
- Specific state changes
- Specific decisions

This makes it far easier to understand **why** the agent is behaving the way it is.

---

### 3. Early Intervention and Safety

One of the most powerful benefits of streaming is **early intervention**.

In long-running or high-stakes workflows, streaming allows:
- Humans to interrupt execution
- Monitoring systems to halt unsafe behavior
- Approval gates to pause the workflow
- Automated safeguards to trigger alerts

Instead of discovering a problem at the end, you can step in **while the agent is still running**.

This is critical for:
- Automation systems
- Decision-support tools
- Financial, legal, or operational workflows
- Any environment where mistakes are costly

---

## Streaming and Control Go Hand in Hand

Streaming complements LangGraphâ€™s philosophy of **explicit control**.

Because:
- Nodes are explicit
- Edges define execution paths
- State is structured and visible

Streaming naturally exposes the internal workings of the agent. Nothing is hidden behind opaque loops or massive prompts.

You are not just seeing outputâ€”you are seeing the **execution of the system itself**.

---

## Streaming as an Observability Layer

In traditional software systems, observability is achieved through:
- Logs
- Metrics
- Traces

In LangGraph, streaming acts as an **observability layer for AI agents**. It provides real-time insight into:
- Reasoning steps
- Actions taken
- Decisions made
- Data flowing through the system

This aligns LangGraph with modern engineering practices, making it suitable for **production-grade AI systems**, not just experiments.

---

## Key Takeaway

Streaming turns LangGraph agents from black boxes into **transparent, observable, and interruptible systems**.

It enhances:
- User trust
- Developer understanding
- System safety
- Operational reliability

In agentic AI, where reasoning unfolds over time, streaming is not a luxuryâ€”it is a **core requirement**.


## Example: Streaming in LangGraph

```python
############################################
# 1. IMPORT REQUIRED LIBRARIES
############################################

# TypedDict and Annotated are used to define a strict, structured state
# This ensures predictable memory across the graph
from typing import TypedDict, Annotated

# add_messages is a LangGraph reducer
# It appends new messages instead of overwriting old ones
from langgraph.graph.message import add_messages

# Core LangGraph classes
from langgraph.graph import StateGraph, START, END

# OpenAI-compatible chat model
from langchain_openai import ChatOpenAI


############################################
# 2. DEFINE THE STATE (AGENT MEMORY)
############################################

# This state represents the agent's memory
# It will flow through the graph and be updated step-by-step
class ChatState(TypedDict):
    # Stores the full conversation history
    # add_messages ensures streaming-friendly message accumulation
    messages: Annotated[list, add_messages]


############################################
# 3. INITIALIZE THE LANGUAGE MODEL (STREAMING ENABLED)
############################################

# Create an LLM instance with streaming enabled
# streaming=True allows token-by-token output
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    streaming=True
)


############################################
# 4. DEFINE THE STREAMING CHATBOT NODE
############################################

def streaming_chatbot_node(state: ChatState):
    """
    This node demonstrates STREAMING behavior.

    What happens here:
    1. The full conversation history is sent to the LLM
    2. The LLM streams tokens as they are generated
    3. We print tokens in real time (live output)
    4. The final response is stored back into state
    """

    print("\nðŸ¤– Assistant (streaming): ", end="", flush=True)

    full_response = ""

    # Stream tokens from the LLM
    for chunk in llm.stream(state["messages"]):
        token = chunk.content
        if token:
            # Print each token as it arrives
            print(token, end="", flush=True)
            full_response += token

    print("\n")  # New line after streaming completes

    # Return the full assistant message to be saved in state
    return {
        "messages": [("assistant", full_response)]
    }


############################################
# 5. BUILD THE LANGGRAPH WORKFLOW
############################################

# Create a StateGraph using the ChatState schema
graph_builder = StateGraph(ChatState)

# Register the streaming chatbot node
graph_builder.add_node("chatbot", streaming_chatbot_node)

# Define execution flow:
# START â†’ chatbot â†’ END
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph into an executable application
app = graph_builder.compile()


############################################
# 6. RUN THE CHATBOT WITH STREAMING
############################################

# Initial state with a user message
initial_state = {
    "messages": [
        ("user", "Explain streaming in LangGraph in simple terms.")
    ]
}

print("\nðŸš€ Starting LangGraph Streaming Execution...\n")

# Stream graph execution events
# This allows observing state updates as they happen
for event in app.stream(initial_state):
    print("ðŸ“¡ STREAM EVENT (STATE UPDATE):")
    print(event)

print("\nâœ… Streaming execution completed.")
```