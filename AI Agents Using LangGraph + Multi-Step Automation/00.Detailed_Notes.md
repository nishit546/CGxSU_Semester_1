# LangGraph: Foundations of Agentic AI Systems

## Introduction to LangGraph

LangGraph is a Python framework built to help developers create **structured, stateful, and controllable AI workflows** using Large Language Models (LLMs). Instead of treating an LLM as a black box that simply generates text in response to a prompt, LangGraph treats AI systems as **explicit graphs of reasoning and action**.

In traditional LLM applications, the workflow is usually linear. A prompt is sent to the model, a response is generated, and the process ends. The model does not remember what happened before, does not understand the broader workflow, and cannot revise or correct itself over time. This makes such systems fragile and unsuitable for complex tasks.

LangGraph introduces a fundamental shift in how AI systems are designed. At its core, it allows you to represent an AI workflow as a **directed graph**, where each step of reasoning or action is explicitly defined. Every operation the AI performs is intentional and visible. Nothing happens implicitly or magically behind the scenes.

Each step in the workflow is represented as a **node**, and the transitions between these steps are represented as **edges**. A shared **state** flows through the graph, carrying memory and context from one step to the next. Because of this structure, AI behavior becomes predictable, debuggable, and safe.

Another critical aspect of LangGraph is **control**. You, as the developer, decide how the AI behaves, when it should stop, when it should loop, and when it should ask for help. This level of control makes LangGraph especially suitable for production systems where reliability, transparency, and safety are essential.

In short, LangGraph is not just a tool for calling LLMs. It is a framework for building **intelligent systems that behave like agents**, not simple chatbots.

---

## Why Agentic AI?

Most AI applications built with LLMs today are **reactive**. They wait for a user prompt, generate a response, and stop. This approach works well for chatbots and simple assistants, but it completely breaks down for complex tasks such as research, planning, automation, and decision-making.

Agentic AI is about building systems that can **act independently toward a goal**. An agentic system does not merely respond to inputâ€”it thinks, decides, and adapts. It can break a large task into smaller steps, evaluate progress, correct mistakes, and continue working until the goal is achieved.

Humans rarely solve complex problems in a single step. We think, check our work, revise our approach, and sometimes ask others for feedback. Agentic AI aims to replicate this problem-solving process in software.

Without an agentic design, AI systems suffer from serious limitations:
- They forget previous steps
- They repeat the same mistakes
- They cannot handle long or multi-stage workflows
- They cannot safely automate important decisions

Agentic AI introduces key concepts that address these issues:
- **Memory**, so the system remembers what it has done
- **Planning**, so it can reason about future steps
- **Tool usage**, so it can act beyond text generation
- **Feedback loops**, so it can improve over time
- **Human oversight**, so critical decisions can be reviewed

LangGraph exists specifically to support agentic AI. It provides the structural foundation required to build systems that can reason over time instead of producing one-off responses.

---

## Why LangGraph Exists

Before LangGraph, developers attempted to build agentic systems using ad-hoc logic or early agent frameworks. As these systems grew more complex, they became increasingly difficult to manage. Execution paths were unclear, debugging was painful, and state management was unreliable.

Many earlier approaches hid critical logic inside prompts or internal loops that were difficult to inspect or control. This made it hard to answer basic questions such as:
- Why did the agent take this action?
- Why did it loop again?
- What information did it use to make this decision?

LangGraph was created to solve these problems by introducing **explicit structure**. Instead of hiding logic, LangGraph forces developers to clearly define:
- What steps exist
- How those steps connect
- What data flows between them

This explicitness is the primary reason LangGraph exists. When everything is defined upfront, the system becomes easier to reason about, easier to debug, and easier to maintain.

Another major reason for LangGraphâ€™s existence is **predictability**. In production environments, unpredictable AI behavior can be dangerous. LangGraph allows developers to define clear rules for execution, including conditional paths, stopping conditions, and human intervention points. This makes AI systems safer and more trustworthy.

LangGraph also supports **long-running workflows**. Many real-world tasks cannot be completed in a single LLM call. LangGraph allows workflows to pause, resume, loop, and wait for human input, all while preserving state.

In essence, LangGraph exists because building serious AI systems requires **engineering discipline**, not just clever prompts.

---

## What Is an Agentic AI Model?

An agentic AI model is not defined by a specific algorithm or neural network. Instead, it is defined by **behavior**. An agentic AI model behaves like an agent: it perceives information, reasons about it, takes actions, and learns from outcomes.

The defining characteristic of an agentic AI model is that it operates over **multiple steps toward a goal**. It does not assume that the first answer is correct. Instead, it evaluates results, adapts its strategy, and continues working.

A typical agentic AI model includes:
- A clearly defined goal or task
- Internal state or memory
- Decision-making logic
- The ability to use tools or external systems
- Feedback mechanisms for improvement

LangGraph enables this behavior by providing a structured environment where these components can exist and interact. The underlying model may still be a standard LLM, but the **system around the model** is what makes it agentic.

This distinction is crucial. LangGraph does not replace LLMs. Instead, it **orchestrates** them. The intelligence of the system emerges from how the model is used across time and steps, not from a single prompt.

---

## Core Components of LangGraph

LangGraph is built on three foundational components:
- **Nodes**
- **Edges**
- **State**

These components work together to form a **directed graph** that defines the AIâ€™s behavior. Understanding these three elements deeply is essential to using LangGraph effectively.

Each component has a clear responsibility:
- Nodes perform work
- Edges control flow
- State carries memory

This separation of concerns is what makes LangGraph powerful, scalable, and maintainable.

---

## Nodes

A node in LangGraph represents a **single unit of work**. Conceptually, a node is one step in the agentâ€™s reasoning or action process. Nodes are implemented as Python functions that receive the current state and return updates to that state.

Nodes should be small and focused. Each node should do **one thing well**. Examples of node responsibilities include:
- Generating a plan
- Calling an LLM
- Using a tool
- Asking a human for input
- Evaluating results

By keeping nodes simple, LangGraph encourages clean and modular architecture. Nodes do not decide what happens next; that responsibility belongs to edges. This separation makes workflows easier to understand, modify, and extend.

Nodes are also easy to test and debug. Because they are plain Python functions, you can run them independently and inspect their inputs and outputs. This is a major advantage over large, monolithic agent loops.

```Python

#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define a structured and predictable state
# This ensures the node knows exactly what data it can access
from typing import TypedDict


#########################################################
# DEFINE THE STATE USED BY THE NODE
#########################################################

class AgentState(TypedDict):
    # Input data that the node will read from
    task: str

    # Output data that the node will write back to state
    result: str


#########################################################
# DEFINE A LANGGRAPH NODE
#########################################################

def node_example(state: AgentState):
    # Read the input task from the shared state
    task = state["task"]

    # Perform a single, focused unit of work
    # In real LangGraph usage, this could be an LLM call or tool execution
    processed_output = f"Processed task inside node: {task}"

    # Return updates to the state
    # LangGraph will merge this into the global state automatically
    return {
        "result": processed_output
    }

```
---

## Edges

Edges define the **control flow** of the graph. They specify which node should execute after another node finishes. Without edges, nodes would exist in isolation with no meaningful execution order.

LangGraph supports multiple types of edges, including direct edges and conditional edges. Conditional edges allow the workflow to branch based on the current state, enabling decision-making and dynamic behavior.

Edges are what make LangGraph truly agentic. They allow the system to:
- Loop until a condition is met
- Choose different paths based on outcomes
- Pause execution for human input
- Terminate safely and intentionally

By making control flow explicit, edges eliminate ambiguity. You can always understand why the system moved from one step to another and under what conditions.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define a structured state
# The edge will use this state to decide control flow
from typing import TypedDict


#########################################################
# DEFINE THE STATE USED BY THE EDGE
#########################################################

class AgentState(TypedDict):
    # Flag that determines which path the graph should take
    approved: bool


#########################################################
# DEFINE AN EDGE (ROUTING FUNCTION)
#########################################################

def edge_router(state: AgentState):
    # Read the decision flag from the shared state
    approved = state["approved"]

    # Conditional logic that controls graph execution flow
    # This function does NOT perform work
    # It only decides which node should run next
    if approved:
        # If condition is met, route execution to the END node
        return "end"

    # If condition is not met, route execution to another node
    return "retry_node"

```
---

## State

State is the **shared memory** of a LangGraph application. It is the data structure that flows through the graph, carrying information from node to node. Without state, the agent would have no memory and no context.

State typically includes:
- The original task or goal
- Intermediate results
- Decisions made by the agent
- Flags used for control flow

LangGraph encourages defining state explicitly, often using typed structures. This makes the system easier to reason about and reduces bugs caused by missing or inconsistent data.

State is what allows LangGraph agents to behave intelligently over time. Each node builds upon what previous nodes have done. This continuity is essential for true agentic behavior.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define a strict and explicit state structure
# This ensures all nodes agree on what data exists in the state
from typing import TypedDict, Annotated

# add_messages is a LangGraph reducer
# It tells LangGraph how to merge new messages into existing state
from langgraph.graph.message import add_messages


#########################################################
# DEFINE THE LANGGRAPH STATE
#########################################################

class AgentState(TypedDict):
    # Stores the full conversation history
    # Annotated + add_messages ensures new messages are appended
    # instead of overwriting previous messages
    messages: Annotated[list, add_messages]

    # Stores a decision flag used for control flow
    # This can be read by edges to route execution
    approved: bool

```
---

## Understanding State Deeply

State is the **most important concept in LangGraph**. If nodes are the brainâ€™s actions and edges are the decision pathways, then **state is the memory and context that gives meaning to everything**. Without state, LangGraph would collapse into a set of disconnected function calls.

In LangGraph, state represents **everything the agent knows at a given moment**. This includes the original task, intermediate reasoning results, tool outputs, decisions, flags, and any other contextual information required to move forward. Unlike traditional LLM applications where context is repeatedly re-sent in prompts, LangGraph maintains state as a structured object that flows naturally through the system.

One of the key advantages of LangGraph state is that it is **explicit**. You define what exists in state upfront, often using typed structures. This makes the system easier to understand and far less error-prone. When you look at the state definition, you can immediately see what information the agent can access and modify.

State also enables **true memory**. Each node receives the current state, performs its work, and returns updates. Those updates are merged back into the state and passed forward. This means later nodes can reason based on everything that happened earlier in the workflow.

Another critical aspect of state is that it enables **branching and looping**. Decisions are not made randomlyâ€”they are made based on values stored in state. Conditional edges inspect the state and determine the next step. This is what allows LangGraph agents to adapt, retry, or stop intelligently.

In production systems, state also becomes a **debugging tool**. You can inspect the state at any point in the graph to understand why a particular decision was made. This level of transparency is one of LangGraphâ€™s strongest features.

In short, state is not just dataâ€”it is the **continuity of intelligence** across the agentâ€™s lifecycle.

---

## Graph API Explained

The Graph API is the **core interface** through which LangGraph applications are built. It is responsible for turning abstract ideas like nodes, edges, and state into a concrete, executable system.

At a high level, the Graph API allows you to:
- Define the structure of the state
- Register nodes
- Connect nodes using edges
- Define entry points
- Compile the graph into a runnable application

The process begins by defining the state schema. This schema acts as a contract for the entire graph. Every node must respect it, and every decision is based on it. Once the state is defined, you create a graph object that knows how to manage that state.

Next, nodes are added to the graph. Each node is given a unique name and mapped to a Python function. The Graph API does not care what the function does internallyâ€”it only cares about how data flows in and out.

Edges are then added to define execution order. This is where the Graph API truly shines. You can define simple linear flows or complex conditional logic that dynamically chooses the next step based on the current state.

Once all nodes and edges are defined, you specify an **entry point**. This tells LangGraph where execution should begin. Finally, the graph is compiled into an executable object.

Compilation is an important step. It validates the graph, ensures that all connections make sense, and prepares the system for execution. After compilation, the graph behaves like a controlled runtime environment for your agent.

The Graph API is intentionally explicit and slightly verbose. This is by design. LangGraph favors clarity and correctness over convenience, making it suitable for serious, production-grade AI systems.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define structured state
from typing import TypedDict, Annotated

# add_messages is a reducer to append new messages
from langgraph.graph.message import add_messages

# StateGraph is the core LangGraph class for building graphs
# START and END define graph entry and exit points
from langgraph.graph import StateGraph, START, END

# Import a language model to use in nodes
from langchain_openai import ChatOpenAI


#########################################################
# DEFINE THE STATE FOR THE GRAPH
#########################################################

class ChatState(TypedDict):
    # Full conversation history stored in state
    messages: Annotated[list, add_messages]
    # Flag used for conditional edges
    approved: bool


#########################################################
# INITIALIZE THE LANGUAGE MODEL
#########################################################

# Create a deterministic LLM instance
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


#########################################################
# DEFINE NODES
#########################################################

def chatbot_node(state: ChatState):
    # Node reads current messages and generates a response
    response = llm.invoke(state["messages"])
    return {"messages": [response], "approved": False}


def human_review_node(state: ChatState):
    # Node asks human for approval
    user_input = input("Approve response? (y/n): ").strip().lower()
    return {"approved": user_input == "y"}


#########################################################
# BUILD THE GRAPH USING GRAPH API
#########################################################

# Create the StateGraph with the defined schema
graph = StateGraph(ChatState)

# Register nodes in the graph
graph.add_node("chatbot", chatbot_node)
graph.add_node("human_review", human_review_node)

# Define linear flow from START to chatbot, then human review
graph.add_edge(START, "chatbot")
graph.add_edge("chatbot", "human_review")

# Define conditional routing after human review
def approval_router(state: ChatState):
    if state["approved"]:
        return "end"      # finish if approved
    return "chatbot"      # loop back if not approved

graph.add_conditional_edges(
    "human_review",
    approval_router,
    {"chatbot": "chatbot", "end": END}
)

# Compile the graph into an executable app
app = graph.compile()


#########################################################
# RUN THE GRAPH
#########################################################

# Initialize state with a user message
initial_state = {
    "messages": [("user", "Explain LangGraph Graph API.")],
    "approved": False
}

# Invoke the graph
final_state = app.invoke(initial_state)

# Print the conversation
for role, message in final_state["messages"]:
    print(f"{role.upper()}: {message}")

```

---

## Tools in LangGraph

Tools are what allow LangGraph agents to **interact with the outside world**. While LLMs are powerful at reasoning and language understanding, they cannot perform actions like searching the web, querying databases, or executing calculations on their own.

In LangGraph, a tool is any external function or capability that the agent can call to perform a specific task. Tools extend the agentâ€™s abilities beyond text generation and turn it into an active problem-solver.

Examples of tools include:
- Web search functions
- Mathematical calculators
- Database query functions
- File system operations
- API calls

Tools are especially important in agentic systems because they allow reasoning and action to work together. The agent can think about what needs to be done, decide which tool is appropriate, use it, and then reason about the result.

LangGraph integrates tools in a structured way. Tool usage is typically handled inside nodes, where the node decides when and how to invoke a tool based on the current state. The output of the tool is then written back into the state so that future nodes can use it.

This design keeps tool usage **explicit and traceable**. You always know when a tool was used, why it was used, and what result it produced. This is critical for debugging, auditing, and safety.

Tools transform LangGraph agents from passive responders into **active agents** capable of real-world interaction. Without tools, agentic AI would remain largely theoretical. With tools, it becomes practical and powerful.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict defines structured state
from typing import TypedDict, Annotated

# add_messages appends messages to conversation state
from langgraph.graph.message import add_messages

# Core LangGraph components
from langgraph.graph import StateGraph, START, END

# Prebuilt tool for basic calculator
from langchain.tools import tool

# Import LLM for node responses
from langchain_openai import ChatOpenAI


#########################################################
# DEFINE THE STATE
#########################################################

class ChatState(TypedDict):
    # Stores conversation history
    messages: Annotated[list, add_messages]

    # Stores output from tools
    tool_result: str


#########################################################
# INITIALIZE THE LLM
#########################################################

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


#########################################################
# DEFINE A CUSTOM TOOL (MULTIPLICATION)
#########################################################

@tool
def multiply_numbers(a: int, b: int) -> str:
    """
    This is a custom LangGraph tool that multiplies two numbers.
    Returns the result as a string to be stored in state.
    """
    result = a * b
    return str(result)


##########

```

---
## Multiple Tools & Tool Routing

In real-world scenarios, an agent rarely relies on a single capability. Instead, it must choose between **multiple tools** depending on the situation. For example, an agent may need to search the web, perform calculations, query a database, or call an external APIâ€”all within the same workflow.

Multiple tools introduce an important challenge: **decision-making**. The agent must decide which tool to use, when to use it, and how to handle the result. This is known as **tool routing**.

In LangGraph, tool routing is not hidden or implicit. Instead, it is explicitly designed into the workflow. Nodes can inspect the current state and decide which tool is appropriate. The result of the tool call is then written back into the state, allowing subsequent nodes to reason about it.

This explicit routing has several advantages:
- You can control which tools are allowed at each step
- You can prevent unsafe or unnecessary tool usage
- You can debug exactly why a tool was chosen

For example, one node might analyze the task and update the state with a decision such as `"use_search": true`. A conditional edge can then route execution to a node that uses a search tool, while skipping it otherwise.

This approach keeps tool usage **deterministic and auditable**, which is critical for production systems. Instead of letting an LLM freely choose tools in an uncontrolled loop, LangGraph ensures that tool decisions are part of the systemâ€™s explicit logic.

Multiple tools and routing transform LangGraph agents from simple assistants into **adaptive problem solvers** capable of handling complex, multi-faceted tasks.

---

## Binding Tools with LLMs

Large Language Models cannot directly execute code or call external systems. They can only generate text. To enable tool usage, tools must be **bound to the LLM** in a way that the model understands how and when they can be used.

Binding tools with LLMs means teaching the model:
- What tools exist
- What each tool does
- What inputs each tool expects
- What outputs each tool returns

In LangGraph, this binding is done explicitly. The LLM is configured with a set of available tools, and the agent logic determines when those tools should be invoked. This separation of concerns is important. The LLM focuses on reasoning, while the graph controls execution.

Binding tools properly ensures that:
- The LLM does not hallucinate tool usage
- Tool calls follow strict schemas
- Outputs are structured and predictable

Another important aspect of binding is **safety**. By controlling which tools are bound and when they are accessible, you prevent the model from taking unintended actions. For example, sensitive tools can be restricted to specific nodes that require human approval.

Binding tools with LLMs is what bridges the gap between **thinking and acting**. Without proper binding, an agent remains theoretical. With binding, it becomes capable of real-world interaction.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict defines structured state for the graph
from typing import TypedDict, Annotated

# add_messages appends new messages to conversation history
from langgraph.graph.message import add_messages

# Core LangGraph components
from langgraph.graph import StateGraph, START, END

# Import LLM for reasoning
from langchain_openai import ChatOpenAI

# Prebuilt tool for demonstration
from langchain.tools import tool, Tool


#########################################################
# DEFINE THE STATE
#########################################################

class ChatState(TypedDict):
    # Stores conversation messages
    messages: Annotated[list, add_messages]

    # Stores tool execution result
    tool_result: str


#########################################################
# INITIALIZE THE LLM
#########################################################

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


#########################################################
# DEFINE A CUSTOM TOOL (MULTIPLICATION)
#########################################################

@tool
def multiply(a: int, b: int) -> str:
    """
    Multiplies two numbers and returns the result as a string.
    """
    return str(a * b)


#########################################################
# DEFINE NODE THAT BINDS TOOL WITH LLM
#########################################################

def tool_binding_node(state: ChatState):
    """
    This node demonstrates binding a tool to the LLM.
    The LLM generates a prompt instructing the tool to run,
    and the node executes the tool and stores output in state.
    """

    # Define a list of tools the LLM can use
    available_tools = [multiply]

    # Example: LLM determines which tool to use (simulated here)
    # Normally the LLM would generate an instruction to call a tool
    # For demonstration, we simulate LLM output as a dictionary
    llm_instruction = {
        "tool_name": "multiply",
        "inputs": {"a": 8, "b": 9}
    }

    # Match the LLM instruction to an actual tool
    for tool_item in available_tools:
        if tool_item.name == llm_instruction["tool_name"]:
            # Call the tool with the inputs provided by the LLM
            result = tool_item(**llm_instruction["inputs"])
            # Store the result in state
            return {"tool_result": result}

    # Fallback if no tool matched
    return {"tool_result": "No tool executed"}


#########################################################
# BUILD THE GRAPH
#########################################################

graph = StateGraph(ChatState)

# Add node
graph.add_node("tool_binding", tool_binding_node)

# Define linear flow
graph.add_edge(START, "tool_binding")
graph.add_edge("tool_binding", END)

# Compile graph
app = graph.compile()


#########################################################
# RUN THE GRAPH
#########################################################

# Initial state
initial_state = {
    "messages": [],
    "tool_result": ""
}

# Execute the graph
final_state = app.invoke(initial_state)

# Print the tool result
print("Tool executed via LLM binding:", final_state["tool_result"])

```

---

## Memory in LangGraph

Memory is what allows an agent to behave intelligently over time. Without memory, every step would be isolated, and the agent would repeat mistakes or lose context. LangGraph treats memory as a first-class concept through its **state system**.

In LangGraph, memory is not hidden inside prompts. Instead, it is stored explicitly in the state and passed through the graph. This makes memory transparent, controllable, and debuggable.

Memory in LangGraph can include:
- Previous decisions
- Intermediate reasoning results
- Tool outputs
- User feedback
- Flags that influence control flow

Because state flows through every node, memory naturally accumulates as the agent progresses. Each node contributes to the agentâ€™s understanding of the task.

This approach to memory has major advantages:
- You can inspect memory at any point
- You can reset or modify memory deliberately
- You can separate reasoning memory from long-term storage

Memory is what transforms LangGraph agents from stateless responders into **context-aware systems** that evolve over time.

---

## Short-Term vs Long-Term Memory

Not all memory is the same. LangGraph workflows often distinguish between **short-term memory** and **long-term memory**, each serving a different purpose.

Short-term memory refers to information that is relevant **only within the current workflow execution**. This includes:
- The current task
- Intermediate results
- Temporary decisions
- Control flags

Short-term memory lives entirely in the graphâ€™s state and is discarded when the workflow ends. It allows the agent to reason coherently within a single run.

Long-term memory, on the other hand, persists beyond a single execution. This might include:
- User preferences
- Historical interactions
- Learned patterns
- Knowledge accumulated over time

LangGraph does not force a specific long-term memory implementation. Instead, it provides the structure needed to integrate external memory systems such as databases, vector stores, or files. Nodes can read from and write to these systems, while references or summaries are stored in state.

This separation is important because it prevents memory overload and keeps workflows efficient. Short-term memory enables reasoning, while long-term memory enables learning.

Together, they allow LangGraph agents to feel both **focused in the moment** and **experienced over time**.

```python
#########################################################
# IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict for structured state
from typing import TypedDict, Annotated

# add_messages reducer appends new messages to conversation history
from langgraph.graph.message import add_messages

# Core LangGraph classes
from langgraph.graph import StateGraph, START, END

# LLM for reasoning
from langchain_openai import ChatOpenAI


#########################################################
# DEFINE STATE WITH MEMORY
#########################################################

class ChatState(TypedDict):
    # Stores full conversation history (short-term memory)
    messages: Annotated[list, add_messages]

    # Long-term memory storage
    long_term_memory: list


#########################################################
# INITIALIZE THE LANGUAGE MODEL
#########################################################

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


#########################################################
# NODE TO ADD SHORT-TERM MEMORY
#########################################################

def add_short_term_memory_node(state: ChatState):
    """
    Node simulating an AI response being added to short-term memory.
    Short-term memory is the message history.
    """
    user_message = ("user", "Remember this short-term info.")
    
    # Return new message to be appended
    return {
        "messages": [user_message]
    }


#########################################################
# NODE TO ADD LONG-TERM MEMORY
#########################################################

def add_long_term_memory_node(state: ChatState):
    """
    Node simulating storing key info in long-term memory.
    Long-term memory persists across sessions or multiple runs.
    """
    new_memory_entry = "Important fact stored in long-term memory."
    
    # Append to long-term memory
    return {
        "long_term_memory": state["long_term_memory"] + [new_memory_entry]
    }


#########################################################
# BUILD THE GRAPH
#########################################################

graph = StateGraph(ChatState)

# Register nodes
graph.add_node("short_term_memory", add_short_term_memory_node)
graph.add_node("long_term_memory", add_long_term_memory_node)

# Linear flow from start â†’ short-term â†’ long-term â†’ end
graph.add_edge(START, "short_term_memory")
graph.add_edge("short_term_memory", "long_term_memory")
graph.add_edge("long_term_memory", END)

# Compile the graph
app = graph.compile()


#########################################################
# RUN THE GRAPH
#########################################################

# Initial state with empty memories
initial_state = {
    "messages": [],
    "long_term_memory": []
}

# Execute the graph
final_state = app.invoke(initial_state)

# Print memory contents
print("Short-term memory (messages):", final_state["messages"])
print("Long-term memory:", final_state["long_term_memory"])

```

---

## Streaming in LangGraph

Streaming is the ability to observe an agentâ€™s output **as it is being generated**, rather than waiting for the entire workflow to complete. This is especially important for long-running or interactive workflows.

In traditional LLM applications, users often wait for a full response before seeing anything. This can feel slow and opaque. Streaming changes this by providing incremental updates as the agent progresses.

In LangGraph, streaming can apply to:
- Token-by-token LLM output
- Step-by-step node execution
- Intermediate state updates

Streaming improves both **user experience and developer visibility**. Users can see progress in real time, while developers can monitor how the agent is reasoning and acting.

Another important benefit of streaming is **early intervention**. If something goes wrong, a human or monitoring system can step in before the workflow completes. This is particularly valuable in high-stakes environments.

Streaming turns LangGraph agents from black boxes into **observable systems**, making them easier to trust and control.

---

## Human-in-the-Loop (HITL)

Human-in-the-Loop (HITL) is a critical concept for building safe and reliable agentic AI systems. While agents can automate many tasks, there are situations where **human judgment is essential**.

LangGraph is designed to support HITL workflows naturally. Human involvement can be introduced at any point in the graph using dedicated nodes and conditional edges. These nodes pause execution, request input or approval, and then update the state based on the humanâ€™s response.

HITL is commonly used for:
- Approval of critical actions
- Review of generated content
- Providing feedback for improvement
- Resolving ambiguity or uncertainty

This approach ensures that agents do not operate blindly. Instead, they collaborate with humans in a controlled and transparent way.

Importantly, HITL does not break the workflow. The graph simply waits until human input is received and then continues execution. This makes LangGraph suitable for complex, real-world systems where autonomy must be balanced with oversight.

Human-in-the-Loop is not a limitationâ€”it is a **design principle** that acknowledges the current boundaries of AI and prioritizes safety and trust.

---

# LangGraph Chatbot:


```python
############################################
# 1. IMPORT REQUIRED LIBRARIES
############################################

# TypedDict is used to define a structured and explicit state
# This ensures that our agent's memory has a fixed and predictable shape
from typing import TypedDict, Annotated

# add_messages is a reducer function provided by LangGraph
# It automatically appends new messages to existing message history
from langgraph.graph.message import add_messages

# StateGraph is the core LangGraph class used to build graphs
from langgraph.graph import StateGraph, START, END

# Import the OpenAI-compatible chat model
# You can replace this with any supported LLM
from langchain_openai import ChatOpenAI


############################################
# 2. DEFINE THE STATE (AGENT MEMORY)
############################################

# This class defines what the agent can remember
# Every node will receive this state and may update it
class ChatState(TypedDict):
    # messages will store the full conversation history
    # Annotated + add_messages ensures new messages are appended automatically
    messages: Annotated[list, add_messages]


############################################
# 3. INITIALIZE THE LANGUAGE MODEL
############################################

# Create an LLM instance
# temperature controls randomness (0 = deterministic)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


############################################
# 4. DEFINE THE CHATBOT NODE
############################################

def chatbot_node(state: ChatState):
    """
    This node represents the 'thinking + responding' step of the chatbot.
    
    It:
    1. Reads the full conversation history from state
    2. Sends it to the LLM
    3. Receives the model's response
    4. Returns the response to be stored back in state
    """

    # Send the conversation history to the LLM
    # The LLM understands message lists (system, user, assistant)
    response = llm.invoke(state["messages"])

    # Return the new message
    # add_messages reducer will append this to existing messages
    return {
        "messages": [response]
    }


############################################
# 5. BUILD THE LANGGRAPH WORKFLOW
############################################

# Create a StateGraph using our ChatState schema
graph_builder = StateGraph(ChatState)

# Register the chatbot node in the graph
# "chatbot" is the node name
graph_builder.add_node("chatbot", chatbot_node)

# Define the execution flow:
# START â†’ chatbot â†’ END
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph into an executable application
app = graph_builder.compile()


############################################
# 6. RUN THE CHATBOT
############################################

# Initial input state
# We start with a user message
initial_state = {
    "messages": [
        ("user", "Hello! Can you explain what LangGraph is?")
    ]
}

# Invoke the graph
# This runs the workflow from START to END
final_state = app.invoke(initial_state)

# Print the final conversation
for role, message in final_state["messages"]:
    print(f"{role.upper()}: {message}")


```

# LangGraph Chatbot with Streaming and Human-in-the-Loop (HITL)


```python
#########################################################
# 1. IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define a strict and explicit state
# This ensures our agent's memory is predictable and safe
from typing import TypedDict, Annotated

# add_messages is a LangGraph reducer
# It automatically appends new messages to existing ones
from langgraph.graph.message import add_messages

# Core LangGraph components
# START and END define graph entry and exit points
from langgraph.graph import StateGraph, START, END

# ChatOpenAI is the LLM wrapper (OpenAI-compatible)
from langchain_openai import ChatOpenAI


#########################################################
# 2. DEFINE THE STATE (AGENT MEMORY)
#########################################################

class ChatState(TypedDict):
    """
    This state represents everything the agent knows.
    It flows through the entire graph.
    """

    # Stores full conversation history
    # Annotated ensures new messages are appended, not overwritten
    messages: Annotated[list, add_messages]

    # Used to track whether a human has approved the response
    approved: bool


#########################################################
# 3. INITIALIZE THE LANGUAGE MODEL
#########################################################

# temperature=0 keeps responses deterministic and stable
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    streaming=True  # Enable streaming at the model level
)


#########################################################
# 4. CHATBOT NODE (WITH STREAMING)
#########################################################

def chatbot_node(state: ChatState):
    """
    This node:
    1. Reads conversation history from state
    2. Streams the LLM response token by token
    3. Collects the full response
    4. Returns it to state
    """

    streamed_response = ""

    # stream() yields chunks of the model output in real time
    for chunk in llm.stream(state["messages"]):
        # Print tokens as they arrive (real-time streaming)
        print(chunk.content, end="", flush=True)

        # Accumulate tokens into a full response
        streamed_response += chunk.content

    print("\n")  # Newline after streaming output finishes

    # Return the assistant message to be stored in state
    return {
        "messages": [("assistant", streamed_response)],
        "approved": False
    }


#########################################################
# 5. HUMAN-IN-THE-LOOP (HITL) NODE
#########################################################

def human_review_node(state: ChatState):
    """
    This node pauses execution and asks a human to:
    - Approve the AI's response
    - Or request a revision
    """

    print("ðŸ¤– AI RESPONSE COMPLETE.")
    print("ðŸ§‘ HUMAN REVIEW REQUIRED.")

    # Ask for explicit human approval
    user_input = input("Approve this response? (y/n): ").strip().lower()

    # Update approval status in state
    return {
        "approved": user_input == "y"
    }


#########################################################
# 6. DECISION FUNCTION (CONDITIONAL EDGE)
#########################################################

def approval_router(state: ChatState):
    """
    This function decides the next step based on state.
    """

    # If approved, finish the workflow
    if state["approved"]:
        return "end"

    # Otherwise, loop back to chatbot for correction
    return "chatbot"


#########################################################
# 7. BUILD THE LANGGRAPH WORKFLOW
#########################################################

# Create the graph using ChatState as schema
graph_builder = StateGraph(ChatState)

# Register nodes
graph_builder.add_node("chatbot", chatbot_node)
graph_builder.add_node("human_review", human_review_node)

# Define execution flow
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", "human_review")

# Conditional routing after human review
graph_builder.add_conditional_edges(
    "human_review",
    approval_router,
    {
        "chatbot": "chatbot",  # Re-run if not approved
        "end": END              # Exit if approved
    }
)

# Compile graph into executable app
app = graph_builder.compile()


#########################################################
# 8. RUN THE CHATBOT
#########################################################

# Initial state with a user message
initial_state = {
    "messages": [
        ("user", "Explain LangGraph in simple terms.")
    ],
    "approved": False
}

# Invoke the graph
# This will:
# - Stream AI response
# - Pause for human approval
# - Loop if rejected
# - End if approved
final_state = app.invoke(initial_state)

# Print final conversation history
print("\nâœ… FINAL CONVERSATION:\n")

for role, message in final_state["messages"]:
    print(f"{role.upper()}: {message}")
```
