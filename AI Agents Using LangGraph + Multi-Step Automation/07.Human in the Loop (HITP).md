# Human-in-the-Loop (HITL)

Human-in-the-Loop (HITL) is a cornerstone concept in **safe, reliable, and production-ready agentic AI systems**. While AI agents can automate reasoning, decision-making, and even action, there are many situations where **human judgment remains essential**.

LangGraph is designed to make HITL workflows **natural, explicit, and controllable**.

---

## How HITL Works in LangGraph

HITL is integrated using **dedicated nodes and conditional edges**. These nodes:

1. Pause execution
2. Request human input, approval, or feedback
3. Update the shared state based on the human response
4. Allow the graph to continue execution safely

This mechanism ensures that agents **never operate blindly**, while maintaining smooth and predictable workflows.

---

## Typical Use Cases for HITL

Human oversight is commonly applied in scenarios such as:

- **Approval of critical actions**  
  E.g., executing payments, changing configurations, or updating records

- **Review of generated content**  
  Ensuring outputs meet quality or compliance standards

- **Providing feedback for improvement**  
  Allowing the agent to learn from human corrections

- **Resolving ambiguity or uncertainty**  
  Letting humans make decisions when the model‚Äôs confidence is low

---

## HITL as a Design Principle

Human-in-the-Loop is **not a limitation**. Instead, it reflects a thoughtful design philosophy that:

- Acknowledges AI‚Äôs current boundaries
- Balances autonomy with safety
- Prioritizes trust and accountability
- Enables collaboration between humans and agents

---

## How HITL Fits Into the Graph

LangGraph treats HITL as **just another node** in the workflow:

- Nodes are explicit
- Edges determine the next steps
- Execution pauses until human input is available
- The state is updated to reflect human decisions

This makes HITL **transparent, auditable, and fully integrated** into the agentic workflow.

---

## Benefits of HITL

- Prevents unsafe or irreversible actions
- Enables quality control for AI-generated outputs
- Provides teachable moments for agent improvement
- Supports regulatory compliance in sensitive domains
- Makes autonomous systems **trustworthy and reliable**

---

## Example Mental Model

Think of HITL as:

> **A safety checkpoint inside your agentic workflow.**  

- The agent thinks and acts, but before critical moves, it stops and asks a human.  
- The human can approve, modify, or reject actions.  
- Once input is provided, the agent continues, confident that the step has human validation.

---

## Key Takeaway

Human-in-the-Loop is essential for **real-world, high-stakes agentic AI systems**.  
LangGraph provides the structure to **pause, consult, and resume** workflows, ensuring safety, transparency, and trust, while still enabling autonomy wherever appropriate.

---

##  Example: Human-in-the-Loop (HITL)

```python
############################################
# 1. IMPORT REQUIRED LIBRARIES
############################################

from typing import TypedDict
from langgraph.graph import StateGraph, START, END

# This will simulate human input in this example
# In real scenarios, this could be replaced with a UI, API, or CLI input
import time


############################################
# 2. DEFINE THE STATE (AGENT MEMORY)
############################################

class AgentState(TypedDict):
    task: str           # The current task or instruction
    approved: bool      # Flag indicating human approval
    result: str         # Result produced by the agent


############################################
# 3. DEFINE NODES
############################################

def agent_step(state: AgentState):
    """
    Simulates a step of the agent that needs human approval.
    """
    # Perform some work
    intermediate_result = f"Agent processed task: {state['task']}"
    print("ü§ñ Agent Step Completed:", intermediate_result)
    
    # Store intermediate result in state
    return {"result": intermediate_result}


def human_approval_node(state: AgentState):
    """
    Pauses execution to get human approval.
    This simulates a Human-in-the-Loop (HITL) step.
    """
    print("üõë Waiting for human approval...")

    # Simulate human taking time to respond
    time.sleep(2)  # In real systems, this would wait for real input
    
    # Simulated approval (replace with real input)
    approved = True
    print(f"‚úÖ Human approved? {approved}")

    # Update state with human decision
    return {"approved": approved}


def final_step(state: AgentState):
    """
    Final step that only runs if human approved the previous action.
    """
    if state.get("approved"):
        result = state["result"] + " ‚Üí Finalized after human approval"
    else:
        result = state["result"] + " ‚Üí Rejected by human"

    print("üèÅ Final Step Result:", result)
    return {"result": result}


############################################
# 4. BUILD THE LANGGRAPH WORKFLOW
############################################

graph = StateGraph(AgentState)

# Register nodes
graph.add_node("agent_step", agent_step)
graph.add_node("human_approval", human_approval_node)
graph.add_node("final_step", final_step)

# Define execution flow
graph.add_edge(START, "agent_step")
graph.add_edge("agent_step", "human_approval")
# Conditional edge based on human approval
graph.add_edge("human_approval", "final_step")
graph.add_edge("final_step", END)

# Compile the graph
app = graph.compile()


############################################
# 5. RUN THE GRAPH
############################################

initial_state = {
    "task": "Perform sensitive operation",
    "approved": False,
    "result": ""
}

final_state = app.invoke(initial_state)

print("\n‚úÖ Final State of Agent:")
print(final_state)
```