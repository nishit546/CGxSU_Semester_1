# LangGraph Chatbot:


```python
############################################
# 1. IMPORT REQUIRED LIBRARIES
############################################

# TypedDict is used to define a structured and explicit state
# This ensures that our agent's memory has a fixed and predictable shape
from typing import TypedDict, Annotated

# add_messages is a reducer function provided by LangGraph
# It automatically appends new messages to existing message history
from langgraph.graph.message import add_messages

# StateGraph is the core LangGraph class used to build graphs
from langgraph.graph import StateGraph, START, END

# Import the OpenAI-compatible chat model
# You can replace this with any supported LLM
from langchain_openai import ChatOpenAI


############################################
# 2. DEFINE THE STATE (AGENT MEMORY)
############################################

# This class defines what the agent can remember
# Every node will receive this state and may update it
class ChatState(TypedDict):
    # messages will store the full conversation history
    # Annotated + add_messages ensures new messages are appended automatically
    messages: Annotated[list, add_messages]


############################################
# 3. INITIALIZE THE LANGUAGE MODEL
############################################

# Create an LLM instance
# temperature controls randomness (0 = deterministic)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)


############################################
# 4. DEFINE THE CHATBOT NODE
############################################

def chatbot_node(state: ChatState):
    """
    This node represents the 'thinking + responding' step of the chatbot.
    
    It:
    1. Reads the full conversation history from state
    2. Sends it to the LLM
    3. Receives the model's response
    4. Returns the response to be stored back in state
    """

    # Send the conversation history to the LLM
    # The LLM understands message lists (system, user, assistant)
    response = llm.invoke(state["messages"])

    # Return the new message
    # add_messages reducer will append this to existing messages
    return {
        "messages": [response]
    }


############################################
# 5. BUILD THE LANGGRAPH WORKFLOW
############################################

# Create a StateGraph using our ChatState schema
graph_builder = StateGraph(ChatState)

# Register the chatbot node in the graph
# "chatbot" is the node name
graph_builder.add_node("chatbot", chatbot_node)

# Define the execution flow:
# START â†’ chatbot â†’ END
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph into an executable application
app = graph_builder.compile()


############################################
# 6. RUN THE CHATBOT
############################################

# Initial input state
# We start with a user message
initial_state = {
    "messages": [
        ("user", "Hello! Can you explain what LangGraph is?")
    ]
}

# Invoke the graph
# This runs the workflow from START to END
final_state = app.invoke(initial_state)

# Print the final conversation
for role, message in final_state["messages"]:
    print(f"{role.upper()}: {message}")


```

# LangGraph Chatbot with Streaming and Human-in-the-Loop (HITL)


```python
#########################################################
# 1. IMPORT REQUIRED LIBRARIES
#########################################################

# TypedDict is used to define a strict and explicit state
# This ensures our agent's memory is predictable and safe
from typing import TypedDict, Annotated

# add_messages is a LangGraph reducer
# It automatically appends new messages to existing ones
from langgraph.graph.message import add_messages

# Core LangGraph components
# START and END define graph entry and exit points
from langgraph.graph import StateGraph, START, END

# ChatOpenAI is the LLM wrapper (OpenAI-compatible)
from langchain_openai import ChatOpenAI


#########################################################
# 2. DEFINE THE STATE (AGENT MEMORY)
#########################################################

class ChatState(TypedDict):
    """
    This state represents everything the agent knows.
    It flows through the entire graph.
    """

    # Stores full conversation history
    # Annotated ensures new messages are appended, not overwritten
    messages: Annotated[list, add_messages]

    # Used to track whether a human has approved the response
    approved: bool


#########################################################
# 3. INITIALIZE THE LANGUAGE MODEL
#########################################################

# temperature=0 keeps responses deterministic and stable
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    streaming=True  # Enable streaming at the model level
)


#########################################################
# 4. CHATBOT NODE (WITH STREAMING)
#########################################################

def chatbot_node(state: ChatState):
    """
    This node:
    1. Reads conversation history from state
    2. Streams the LLM response token by token
    3. Collects the full response
    4. Returns it to state
    """

    streamed_response = ""

    # stream() yields chunks of the model output in real time
    for chunk in llm.stream(state["messages"]):
        # Print tokens as they arrive (real-time streaming)
        print(chunk.content, end="", flush=True)

        # Accumulate tokens into a full response
        streamed_response += chunk.content

    print("\n")  # Newline after streaming output finishes

    # Return the assistant message to be stored in state
    return {
        "messages": [("assistant", streamed_response)],
        "approved": False
    }


#########################################################
# 5. HUMAN-IN-THE-LOOP (HITL) NODE
#########################################################

def human_review_node(state: ChatState):
    """
    This node pauses execution and asks a human to:
    - Approve the AI's response
    - Or request a revision
    """

    print("ðŸ¤– AI RESPONSE COMPLETE.")
    print("ðŸ§‘ HUMAN REVIEW REQUIRED.")

    # Ask for explicit human approval
    user_input = input("Approve this response? (y/n): ").strip().lower()

    # Update approval status in state
    return {
        "approved": user_input == "y"
    }


#########################################################
# 6. DECISION FUNCTION (CONDITIONAL EDGE)
#########################################################

def approval_router(state: ChatState):
    """
    This function decides the next step based on state.
    """

    # If approved, finish the workflow
    if state["approved"]:
        return "end"

    # Otherwise, loop back to chatbot for correction
    return "chatbot"


#########################################################
# 7. BUILD THE LANGGRAPH WORKFLOW
#########################################################

# Create the graph using ChatState as schema
graph_builder = StateGraph(ChatState)

# Register nodes
graph_builder.add_node("chatbot", chatbot_node)
graph_builder.add_node("human_review", human_review_node)

# Define execution flow
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", "human_review")

# Conditional routing after human review
graph_builder.add_conditional_edges(
    "human_review",
    approval_router,
    {
        "chatbot": "chatbot",  # Re-run if not approved
        "end": END              # Exit if approved
    }
)

# Compile graph into executable app
app = graph_builder.compile()


#########################################################
# 8. RUN THE CHATBOT
#########################################################

# Initial state with a user message
initial_state = {
    "messages": [
        ("user", "Explain LangGraph in simple terms.")
    ],
    "approved": False
}

# Invoke the graph
# This will:
# - Stream AI response
# - Pause for human approval
# - Loop if rejected
# - End if approved
final_state = app.invoke(initial_state)

# Print final conversation history
print("\nâœ… FINAL CONVERSATION:\n")

for role, message in final_state["messages"]:
    print(f"{role.upper()}: {message}")
```